# Optimized training config for 2x24GB GPUs (48GB total)
# Memory-efficient settings while maintaining training quality

reward_training:
  # Reward model training (Phase 1)
  # Qwen3-4B + small batch fits in single 24GB GPU
  batch_size: 8  # Reduced from 16 to fit in memory
  learning_rate: 2e-5
  num_epochs: 3
  warmup_ratio: 0.1
  weight_decay: 0.01
  max_examples: 50000  # Full dataset
  num_incorrect_per_problem: 3
  gradient_accumulation_steps: 2  # Effective batch size = 8 * 2 = 16

policy_training:
  # Policy network training (Phase 2)
  # GPU 0: Qwen3-8B generator (~16GB)
  # GPU 1: Qwen3-4B reward model (~8GB) + GPS policy (~2GB)
  algorithm: "ppo"
  batch_size: 4  # Reduced from 8 - critical for memory
  learning_rate: 1e-5
  num_iterations: 100
  problems_per_iteration: 16  # Reduced from 32 for faster iteration
  ppo_epochs: 4
  clip_epsilon: 0.2
  value_coeff: 0.5
  entropy_coeff: 0.01
  max_grad_norm: 1.0

  # Reward shaping
  final_reward_correct: 10.0
  final_reward_incorrect: -5.0
  step_penalty: -0.1

  # GAE parameters
  gae_lambda: 0.95
  discount_gamma: 0.99

search:
  # Search parameters during training
  max_depth: 15  # Reduced from 20 to save memory
  max_nodes: 80  # Reduced from 100
  temperature: 0.8
  use_policy: true

# Memory optimization flags
optimization:
  use_gradient_checkpointing: true
  use_fp16: true  # Mixed precision training
  offload_optimizer: false  # Set to true if OOM
  max_sequence_length: 512  # Truncate long sequences
